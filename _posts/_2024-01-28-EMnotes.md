---
layout: post
usemathjax: true
categories: ['embracing transitory confusion', 'probability theory', math]
title: "Notes on generative models, VAE, and EM (estimation-maximization)"
permalink: /:year/:title:output_ext


# note: shift-cmd-v to view the markdown view; cmd-k v to view side-by-side, then can do 'toggle preview locking' command in the 3 dots in the preview tab

# note: replace \b with \mathbf, if I decide to use \mathbf
---
Some background context
----------

These notes refer to the [Calvin Luo paper on understanding diffusion models](https://arxiv.org/pdf/2208.11970.pdf){:target="_blank"} and the ....lectures...

Let $$x$$ be a vector in a space of images, and $$z$$ be a vector in a latent space of image classifications. 

For example, if each image has $$n^2=n \cdot n$$ pixels and each pixel has 3 numbers representing RGB values, then $$x$$ could be a 1-dimensional vector of length $$3n^2$$ elements, where each element is a number. $$z$$ could be a multidimensional vector with more complex structure, depending on the nature of the classifications.

Then $$P(x \mid z)$$ would be a distribution over all the $$x$$ that satisfy the constraint $$z$$. The peak of this distribution is for $$x$$ that very strongly satisfies $$z$$; the tail of the distribution is for $$x$$ that weakly satisfies $$z$$.

For example, suppose $$z_0$$ represents "human face". Then $$x_1$$ corresponding to "face of a child" would strongly satisfy $$z_0$$, while $$x_2$$ corresponding to "face of a statue of a child" would weakly satisfy $$z_0$$, and $$x_3$$ corresponding to "a sunny beach" would not at all satisfy $$z_0$$.

Let $$P(x,z)$$ be the joint probability density of $$x$$ and $$z$$. If we sample $$(x,z) \sim P(x,z)$$, then each $$(x,z)$$ sample would be an image together with its latent classification. But the image and its classification would be randomly chosen, with the most probable image/classification pair most frequently sampled.

So if we instead want to generate images with a specific classification (e.g., "human face"), we need to fix $$z$$ and randomly sample $$x \sim P(x \mid z)$$.

Confusion: do we need to know $$P(z)$$?
----
We know that $$P(x \mid z) = P(x,z)/P(z)$$. So it appears that in order to calculate the conditional probability, we not only need to know the joint density $$P(x,z)$$, but also need to know $$P(z)$$. So if we want to generate human faces and choose the appropriate $$z=z_0$$, what does $$P(z)$$ mean?

One way to think of this is to consider $$P(z)$$ to be a delta function at $$z_0$$, or more generally a very narrow Gaussian that is centered at $$z_0$$. But a clearer way to think of this is to realize that $$P(x \mid z)$$ is a probability density of $$x$$. And therefore it must normalize to $$1$$ when we integrate over all values of $$x$$. And so the $$P(z)$$ in the denominator is there to ensure the normalization. But if we use a sampling method that does not require normalization, for example MCMC, then we don't care about this denominator and don't need $$P(z)$$. And then we can just think of this as an undetermined normalization factor of $$1/P(z)$$.

Learning the model
------
We want to create an approximate model of the probability distribution $$P(x \mid z)$$; to do so, we construct a variational function that depends on parameters $$\theta$$ and apply maximum likelihood estimation to learn the optimal $$\theta$$. So we need to choose $$\theta$$ to maximize the likelihood function $$P(x \mid z ; \theta)$$ over the training set $${x}$$. Concretely, we maximize the sum of the log likelihoods over the training set of images.



---

[Share or comment on Mastodon](https://hachyderm.io/@Sunfishstanford/111677727136310287){:target="_blank"}

[//]: # (Bing prompt: Convert the following text to latex format,  only putting the math equation parts between the latex delimiters, and using $$ for the latex delimiters for both math mode and display math mode.)
